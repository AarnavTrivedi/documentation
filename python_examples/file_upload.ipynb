{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trieve PDF Upload With Default Chunking Example\n",
    "\n",
    "At Trieve, we have a default chunking algorithm that splits documents based on converting them to HTML with Apache Tika and then splitting on headings or recursively when chunks get too large. You can look at the code for our default chunking approach [here](https://github.com/devflowinc/trieve/blob/main/server/server-python/chunker.py). It serves as a sane default for the majority of use-cases. \n",
    "\n",
    "However, we do strongly encourage you to invest in chunking your own data. In our opinion, this is the most valuable thing you can do for increasing the performance of your RAG or search pipeline. Feel free to join the community on [Discord](https://discord.gg/E9sPRZqpDT) or [Matrix](https://matrix.to/#/#trieve-general:matrix.zerodao.gg) to get help from Trieve core developers directly. We are always happy to help! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download a PDF to upload to Trieve\n",
    "\n",
    "For this example, we will use the famous \"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\" paper which can be found [here](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922). It's a very interesting publication if you have already read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF has been downloaded and saved to ./tmp/stochastic_parrots.pdf\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def download_and_save_pdf(url, save_path):\n",
    "    # Ensure the save_path directory exists, if not, create it.\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    # Get the PDF content from the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Open the target file in binary write mode and save the PDF content\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f'PDF has been downloaded and saved to {save_path}')\n",
    "    else:\n",
    "        print(f'Failed to download PDF. Status code: {response.status_code}')\n",
    "\n",
    "# Example usage\n",
    "pdf_url = 'https://dl.acm.org/doi/pdf/10.1145/3442188.3445922'\n",
    "save_path = './tmp/stochastic_parrots.pdf'\n",
    "download_and_save_pdf(pdf_url, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get an API Key For Trieve\n",
    "\n",
    "1. Sign up for a free tier account at [dashboard.trieve.ai](https://dashboard.trieve.ai) which comes with 512mb of free file storage\n",
    "2. Create a new dataset\n",
    "3. Create a `Read - Write` level API Key\n",
    "\n",
    "Finally, copy and paste your API key and dataset ID into the relevant values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trieve_dataset_id = \"2330f358-3ad5-485f-b83e-351496128859\"\n",
    "trieve_api_key = \"tr-rtxQmkhSPe06hCDNynCe9DBbbRrSpVbA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install and import the trieve client python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting trieve_client_py\n",
      "  Using cached trieve_client_py-0.3.21-py3-none-any.whl (171 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /home/skeptrune/.local/lib/python3.10/site-packages (from trieve_client_py) (2.8.2)\n",
      "Requirement already satisfied: attrs>=21.3.0 in /home/skeptrune/.local/lib/python3.10/site-packages (from trieve_client_py) (23.1.0)\n",
      "Requirement already satisfied: httpx<0.27.0,>=0.20.0 in /home/skeptrune/.local/lib/python3.10/site-packages (from trieve_client_py) (0.24.1)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /home/skeptrune/.local/lib/python3.10/site-packages (from httpx<0.27.0,>=0.20.0->trieve_client_py) (0.17.3)\n",
      "Requirement already satisfied: sniffio in /home/skeptrune/.local/lib/python3.10/site-packages (from httpx<0.27.0,>=0.20.0->trieve_client_py) (1.3.0)\n",
      "Requirement already satisfied: certifi in /home/skeptrune/.local/lib/python3.10/site-packages (from httpx<0.27.0,>=0.20.0->trieve_client_py) (2023.7.22)\n",
      "Requirement already satisfied: idna in /home/skeptrune/.local/lib/python3.10/site-packages (from httpx<0.27.0,>=0.20.0->trieve_client_py) (3.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.8.0->trieve_client_py) (1.16.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/skeptrune/.local/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<0.27.0,>=0.20.0->trieve_client_py) (3.7.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/skeptrune/.local/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<0.27.0,>=0.20.0->trieve_client_py) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/skeptrune/.local/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx<0.27.0,>=0.20.0->trieve_client_py) (1.1.3)\n",
      "Installing collected packages: trieve_client_py\n",
      "Successfully installed trieve_client_py-0.3.21\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install trieve_client_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize an authenticated trieve_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trieve_client import AuthenticatedClient\n",
    "\n",
    "client = AuthenticatedClient(\n",
    "    base_url=\"https://api.trieve.ai\", prefix=\"\", token=trieve_api_key\n",
    ").with_headers({\"TR-Dataset\": trieve_dataset_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. base64 encode your file to prepare it for upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def to_base64(file):\n",
    "    try:\n",
    "        with open(file, 'rb') as f:\n",
    "            return base64.b64encode(f.read()).decode('utf-8')  \n",
    "    except Exception as error:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "        return None\n",
    "\n",
    "base64_file = to_base64(save_path or \"\")\n",
    "\n",
    "if base64_file:\n",
    "    base64_file = base64_file.replace(\"+\", \"-\")  # Convert '+' to '-'\n",
    "    base64_file = base64_file.replace(\"/\", \"_\")  # Convert '/' to '_'\n",
    "    base64_file = base64_file.rstrip('=')  # Remove ending '='\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Send your file to Trieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'user_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrieve_client\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upload_file_handler\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrieve_client\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merror_response_body\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ErrorResponseBody\n\u001b[0;32m----> 5\u001b[0m file_upload_response \u001b[38;5;241m=\u001b[39m \u001b[43mupload_file_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtr_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrieve_dataset_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUploadFileData\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstochastic_parrots.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfile_mime_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbase64_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase64_file\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(file_upload_response) \u001b[38;5;241m==\u001b[39m UploadFileResult:\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile has been chunked and chunks have been queued for indexing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/trieve_client/api/file/upload_file_handler.py:132\u001b[0m, in \u001b[0;36msync\u001b[0;34m(client, body, tr_dataset)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msync\u001b[39m(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    106\u001b[0m     client: AuthenticatedClient,\n\u001b[1;32m    107\u001b[0m     body: UploadFileData,\n\u001b[1;32m    108\u001b[0m     tr_dataset: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    109\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[ErrorResponseBody, UploadFileResult]]:\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"upload_file\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m     upload_file\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m        Union[ErrorResponseBody, UploadFileResult]\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync_detailed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtr_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtr_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mparsed\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/trieve_client/api/file/upload_file_handler.py:101\u001b[0m, in \u001b[0;36msync_detailed\u001b[0;34m(client, body, tr_dataset)\u001b[0m\n\u001b[1;32m     92\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m _get_kwargs(\n\u001b[1;32m     93\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m     94\u001b[0m     tr_dataset\u001b[38;5;241m=\u001b[39mtr_dataset,\n\u001b[1;32m     95\u001b[0m )\n\u001b[1;32m     97\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_httpx_client()\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     99\u001b[0m )\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_build_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/trieve_client/api/file/upload_file_handler.py:60\u001b[0m, in \u001b[0;36m_build_response\u001b[0;34m(client, response)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_response\u001b[39m(\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;241m*\u001b[39m, client: Union[AuthenticatedClient, Client], response: httpx\u001b[38;5;241m.\u001b[39mResponse\n\u001b[1;32m     55\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response[Union[ErrorResponseBody, UploadFileResult]]:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m     57\u001b[0m         status_code\u001b[38;5;241m=\u001b[39mHTTPStatus(response\u001b[38;5;241m.\u001b[39mstatus_code),\n\u001b[1;32m     58\u001b[0m         content\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[1;32m     59\u001b[0m         headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m---> 60\u001b[0m         parsed\u001b[38;5;241m=\u001b[39m\u001b[43m_parse_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/trieve_client/api/file/upload_file_handler.py:40\u001b[0m, in \u001b[0;36m_parse_response\u001b[0;34m(client, response)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_response\u001b[39m(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;241m*\u001b[39m, client: Union[AuthenticatedClient, Client], response: httpx\u001b[38;5;241m.\u001b[39mResponse\n\u001b[1;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[ErrorResponseBody, UploadFileResult]]:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m HTTPStatus\u001b[38;5;241m.\u001b[39mOK:\n\u001b[0;32m---> 40\u001b[0m         response_200 \u001b[38;5;241m=\u001b[39m \u001b[43mUploadFileResult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response_200\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m HTTPStatus\u001b[38;5;241m.\u001b[39mBAD_REQUEST:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/trieve_client/models/upload_file_result.py:41\u001b[0m, in \u001b[0;36mUploadFileResult.from_dict\u001b[0;34m(cls, src_dict)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m File\n\u001b[1;32m     40\u001b[0m d \u001b[38;5;241m=\u001b[39m src_dict\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 41\u001b[0m file_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile_metadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m upload_file_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m     44\u001b[0m     file_metadata\u001b[38;5;241m=\u001b[39mfile_metadata,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m upload_file_result\u001b[38;5;241m.\u001b[39madditional_properties \u001b[38;5;241m=\u001b[39m d\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/trieve_client/models/file.py:119\u001b[0m, in \u001b[0;36mFile.from_dict\u001b[0;34m(cls, src_dict)\u001b[0m\n\u001b[1;32m    115\u001b[0m size \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m updated_at \u001b[38;5;241m=\u001b[39m isoparse(d\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdated_at\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 119\u001b[0m user_id \u001b[38;5;241m=\u001b[39m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_link\u001b[39m(data: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28;01mNone\u001b[39;00m, Unset, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'user_id'"
     ]
    }
   ],
   "source": [
    "from trieve_client.models import UploadFileData, UploadFileResult\n",
    "from trieve_client.api.file import upload_file_handler\n",
    "from trieve_client.models.error_response_body import ErrorResponseBody\n",
    "\n",
    "file_upload_response = upload_file_handler.sync(\n",
    "    client=client,\n",
    "    tr_dataset=trieve_dataset_id,\n",
    "    body=UploadFileData(\n",
    "      file_name=\"stochastic_parrots.pdf\",\n",
    "      file_mime_type=\"application/pdf\",\n",
    "      base64_file=base64_file\n",
    "    )\n",
    ")\n",
    "\n",
    "if type(file_upload_response) == UploadFileResult:\n",
    "  print(\"File has been chunked and chunks have been queued for indexing\")\n",
    "  uploaded_file_id = file_upload_response.file_metadata.id\n",
    "  print(f\"File ID: {file_upload_response.file_metadata}\")\n",
    "elif type(file_upload_response) == ErrorResponseBody:\n",
    "  print(f\"Failed to upload file {file_upload_response.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Get the chunk_group created with your file's chunks\n",
    "\n",
    "#### NOTE: Soon we plan to revamp Trieve's [get_events](https://api.trieve.ai/redoc#tag/events/operation/get_events) route with webhook support and other niceities so this hacky approach is no longer necessary \n",
    "\n",
    "When a file is uploaded all of its chunks are added to a `chunk_group` within our data schema. To check all of the files you have uploaded, you can paginate through the available `chunk_groups`. \n",
    "\n",
    "Below is an example of doing that to locate the `chunk_group` for the file you just uploaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d97f9b44-748d-4a93-ac38-b5b9bdb58bae'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trieve_client.api.chunk_group import get_specific_dataset_chunk_groups\n",
    "from trieve_client.models.group_data import GroupData\n",
    "\n",
    "page = 1\n",
    "file_group_id = None\n",
    "\n",
    "while file_group_id is None:\n",
    "  chunk_groups_response = get_specific_dataset_chunk_groups.sync(\n",
    "      client=client,\n",
    "      tr_dataset=trieve_dataset_id,\n",
    "      dataset_id=trieve_dataset_id,\n",
    "      page=page,\n",
    "  )\n",
    "  if type(chunk_groups_response) == GroupData:\n",
    "    groups = chunk_groups_response.groups\n",
    "    for group in groups:\n",
    "      if group.file_id == uploaded_file_id:\n",
    "        file_group_id = group.id\n",
    "        break\n",
    "\n",
    "    if len(groups) == 10:   \n",
    "      page += 1\n",
    "  else:\n",
    "    print(f\"Failed to fetch chunk groups: {chunk_groups_response.message}\")\n",
    "\n",
    "file_group_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Semantic Search Within the File's chunk_group\n",
    "\n",
    "Beyond searching the whole dataset with the [search_chunk](https://api.trieve.ai/redoc#tag/chunk/operation/search_chunk) route, you also have the ability to search within only the subset of chunks inside of a `chunk_group`. \n",
    "\n",
    "In the context of this example, those will be the chunks belonging to the `stochastic_parrots.pdf` paper. \n",
    "\n",
    "Also, please be aware that all of this functionality is exposed via Trieve's admin tooling UI's at [search.trieve.ai](https://search.trieve.ai) and [chat.trieve.ai](https://chat.trieve.ai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ScoreChunkDTO(metadata=[ChunkMetadataWithFileData(content=' FAccT ’21, March 3–10, 2021, Virtual Event, Canada Bender and Gebru, et al. in [21, 93] and direct resources away from efforts that would facili- tate long-term progress towards natural language understanding, without using unfathomable training data. Furthermore, the tendency of human interlocutors to impute meaning where there is none can mislead both NLP researchers and the general public into taking synthetic text as meaningful. Combined with the ability of LMs to pick up on both subtle biases and overtly abusive language patterns in training data, this leads to risks of harms, including encountering derogatory language and experiencing discrimination at the hands of others who reproduce racist, sexist, ableist, extremist or other harmful ideologies rein- forced through interactions with synthetic language. We explore these potential harms in §6 and potential paths forward in §7. We hope that a critical overview of the risks of relying on ever- increasing size of LMs as the primary driver of increased perfor- mance of language technology can facilitate a reallocation of efforts towards approaches that avoid some of these risks while still reap- ing the benefits of improvements to language technology. 2 BACKGROUND Similar to [14], we understand the term language model (LM) to refer to systems which are trained on string prediction tasks: that is, predicting the likelihood of a token (character, word or string) given either its preceding context or (in bidirectional and masked LMs) its surrounding context. Such systems are unsupervised and when deployed, take a text as input, commonly outputting scores or string predictions. Initially proposed by Shannon in 1949 [117], some of the earliest implemented LMs date to the early 1980s and were used as components in systems for automatic speech recognition (ASR), machine translation (MT), document classification, and more [111]. In this section, we provide a brief overview of the general trend of language modeling in recent years. For a more in-depth survey of pretrained LMs, see [105]. Before neural models, n-gram models also used large amounts of data [20, 87].', created_at=datetime.datetime(2024, 2, 14, 8, 52, 43, 67779), id='e379735d-c442-445f-aa86-ba4ba4e629ca', qdrant_point_id='691a249f-d157-42cc-880b-2156054cb5c3', updated_at=datetime.datetime(2024, 2, 14, 8, 52, 43, 67781), weight=1.0, chunk_html='<div><div> FAccT ’21, March 3–10, 2021, Virtual Event, Canada Bender and Gebru, et al. in [21, 93] and direct resources away from efforts that would facili- tate long-term progress towards natural language understanding, without using unfathomable training data. Furthermore, the tendency of human interlocutors to impute meaning where there is none can mislead both NLP researchers and the general public into taking synthetic text as meaningful. Combined with the ability of LMs to pick up on both subtle biases and overtly abusive language patterns in training data, this leads to risks of harms,<b> including encountering derogatory language and experiencing discrimination at the hands of others who reproduce racist,</b> sexist, ableist, extremist or other harmful ideologies rein- forced through interactions with synthetic language. We explore these potential harms in §6 and potential paths forward in §7.<b> We hope that a critical overview of the risks of relying on ever- increasing size of LMs as the primary driver of increased perfor- mance of language technology can facilitate a reallocation of efforts towards approaches that avoid some of these risks while still reap- ing the benefits of improvements to language technology.</b> 2 BACKGROUND Similar to [14], we understand the term language model (LM) to refer to systems which are trained on string prediction tasks: that is, predicting the likelihood of a token (character, word or string) given either its preceding context or (in bidirectional and masked LMs) its surrounding context. Such systems are unsupervised and when deployed, take a text as input, commonly outputting scores or string predictions. Initially proposed by Shannon in 1949 [117],<b> some of the earliest implemented LMs date to the early 1980s and were used as components in systems for automatic speech recognition (ASR),</b> machine translation (MT), document classification, and more [111]. In this section, we provide a brief overview of the general trend of language modeling in recent years. For a more in-depth survey of pretrained LMs, see [105]. Before neural models, n-gram models also used large amounts of data [20, 87].</div></div>', file_id='05eebb93-b71a-42a6-99c3-ae0c546c4f12', file_name='stochastic_parrots.pdf', link=None, metadata={'Content-Length': '304490', 'Content-Type': 'application/pdf', 'PDFVersion': '1.5', 'PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'X-TIKA:Parsed-By': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'X-TIKA:Parsed-By-Full-Set': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'access_permission:assemble_document': 'true', 'access_permission:can_modify': 'true', 'access_permission:can_print': 'true', 'access_permission:can_print_degraded': 'true', 'access_permission:extract_content': 'true', 'access_permission:extract_for_accessibility': 'true', 'access_permission:fill_in_form': 'true', 'access_permission:modify_annotations': 'true', 'dc:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'dc:format': 'application/pdf; version=1.6', 'dc:language': 'en', 'dc:subject': '-  Computing methodologies  ->  Natural language processing.', 'dc:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'dcterms:created': '2021-02-09T18:04:12Z', 'dcterms:modified': '2024-02-14T08:52:36Z', 'language': 'en', 'pdf:PDFVersion': '1.6', 'pdf:annotationSubtypes': 'Link', 'pdf:annotationTypes': 'null', 'pdf:charsPerPage': ['4262', '5546', '5999', '6061', '5888', '5961', '5806', '6061', '6022', '6021', '8947', '9205', '9288', '7753'], 'pdf:containsDamagedFont': 'false', 'pdf:containsNonEmbeddedFont': 'false', 'pdf:docinfo:created': '2021-02-09T18:04:12Z', 'pdf:docinfo:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'pdf:docinfo:creator_tool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'pdf:docinfo:custom:PDFVersion': '1.5', 'pdf:docinfo:custom:PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'pdf:docinfo:modified': '2024-02-14T08:52:36Z', 'pdf:docinfo:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:docinfo:subject': '-  Computing methodologies  ->  Natural language processing.', 'pdf:docinfo:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'pdf:docinfo:trapped': 'False', 'pdf:encrypted': 'false', 'pdf:hasCollection': 'false', 'pdf:hasMarkedContent': 'false', 'pdf:hasXFA': 'false', 'pdf:hasXMP': 'true', 'pdf:num3DAnnotations': '0', 'pdf:overallPercentageUnmappedUnicodeChars': '7.54147840780206E-5', 'pdf:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:totalUnmappedUnicodeChars': '7', 'pdf:unmappedUnicodeCharsPerPage': ['0', '0', '7', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], 'xmp:CreatorTool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'xmpTPg:NPages': '14'}, tag_set=None, time_stamp=None, tracking_id=None, additional_properties={})], score=0.7915455102920532, additional_properties={}), ScoreChunkDTO(metadata=[ChunkMetadataWithFileData(content=' Stochastic Parrots FAccT ’21, March 3–10, 2021, Virtual Event, Canada mBERT across 29 tasks. Either way, these models do not address the inclusion problems raised by [65], who note that over 90% of the world’s languages used by more than a billion people currently have little to no support in terms of language technology. Alongside work investigating what information the models re- tain from the data, we see a trend in reducing the size of these models using various techniques such as knowledge distillation [26, 58], quantization [118, 153], factorized embedding parame- terization and cross-layer parameter sharing [70], and progressive module replacing [146]. Rogers et al. [110] provide a comprehensive comparison of models derived from BERT using these techniques, such as DistilBERT [113] and ALBERT [70]. While these models maintain and sometimes exceed the performance of the original BERT model, despite their much smaller size, they ultimately still rely on large quantities of data and significant processing and stor- age capabilities to both hold and reduce the model. We note that the change from n-gram LMs to word vectors dis- tilled from neural LMs to pretrained Transformer LMs is paralleled by an expansion and change in the types of tasks they are use- ful for: n-gram LMs were initially typically deployed in selecting among the outputs of e. acoustical or translation models; the LSTM-derived word vectors were quickly picked up as more effec- tive representations of words (in place of bag of words features) in a variety of NLP tasks involving labeling and classification; and the pretrained Transformer models can be retrained on very small datasets (few-shot, one-shot or even zero-shot learning) to perform apparently meaning-manipulating tasks such as summarization, question answering and the like. Nonetheless, all of these systems share the property of being LMs in the sense we give above, that is, systems trained to predict sequences of words (or characters or sentences).', created_at=datetime.datetime(2024, 2, 14, 8, 52, 43, 943657), id='fa54ea57-62d3-4a21-b181-f4b60c69752d', qdrant_point_id='0f583282-1038-4938-b657-4cdae16c71cb', updated_at=datetime.datetime(2024, 2, 14, 8, 52, 43, 943659), weight=1.0, chunk_html='<div><div> Stochastic Parrots FAccT ’21, March 3–10, 2021, Virtual Event, Canada mBERT across 29 tasks. Either way, these models do not address the inclusion problems raised by [65], who note that over 90% of the world’s languages used by more than a billion people currently have little to no support in terms of language technology. Alongside work investigating what information the models re- tain from the data, we see a trend in reducing the size of these models using various techniques such as knowledge distillation [26, 58], quantization [118, 153], factorized embedding parame- terization and cross-layer parameter sharing [70], and progressive module replacing [146]. Rogers et al. [110] provide a comprehensive comparison of models derived from BERT using these techniques, such as DistilBERT [113] and ALBERT [70]. While these models maintain and sometimes exceed the performance of the original BERT model, despite their much smaller size,<b> they ultimately still rely on large quantities of data and significant processing and stor- age capabilities to both hold and reduce the model.</b><b> We note that the change from n-gram LMs to word vectors dis- tilled from neural LMs to pretrained Transformer LMs is paralleled by an expansion and change in the types of tasks they are use- ful for: n-gram LMs were initially typically deployed in selecting among the outputs of e.</b><b> acoustical or translation models; the LSTM-derived word vectors were quickly picked up as more effec- tive representations of words (in place of bag of words features) in a variety of NLP tasks involving labeling and classification; and the pretrained Transformer models can be retrained on very small datasets (few-shot,</b> one-shot or even zero-shot learning) to perform apparently meaning-manipulating tasks such as summarization, question answering and the like. Nonetheless, all of these systems share the property of being LMs in the sense we give above, that is, systems trained to predict sequences of words (or characters or sentences).</div></div>', file_id='05eebb93-b71a-42a6-99c3-ae0c546c4f12', file_name='stochastic_parrots.pdf', link=None, metadata={'Content-Length': '304490', 'Content-Type': 'application/pdf', 'PDFVersion': '1.5', 'PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'X-TIKA:Parsed-By': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'X-TIKA:Parsed-By-Full-Set': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'access_permission:assemble_document': 'true', 'access_permission:can_modify': 'true', 'access_permission:can_print': 'true', 'access_permission:can_print_degraded': 'true', 'access_permission:extract_content': 'true', 'access_permission:extract_for_accessibility': 'true', 'access_permission:fill_in_form': 'true', 'access_permission:modify_annotations': 'true', 'dc:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'dc:format': 'application/pdf; version=1.6', 'dc:language': 'en', 'dc:subject': '-  Computing methodologies  ->  Natural language processing.', 'dc:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'dcterms:created': '2021-02-09T18:04:12Z', 'dcterms:modified': '2024-02-14T08:52:36Z', 'language': 'en', 'pdf:PDFVersion': '1.6', 'pdf:annotationSubtypes': 'Link', 'pdf:annotationTypes': 'null', 'pdf:charsPerPage': ['4262', '5546', '5999', '6061', '5888', '5961', '5806', '6061', '6022', '6021', '8947', '9205', '9288', '7753'], 'pdf:containsDamagedFont': 'false', 'pdf:containsNonEmbeddedFont': 'false', 'pdf:docinfo:created': '2021-02-09T18:04:12Z', 'pdf:docinfo:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'pdf:docinfo:creator_tool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'pdf:docinfo:custom:PDFVersion': '1.5', 'pdf:docinfo:custom:PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'pdf:docinfo:modified': '2024-02-14T08:52:36Z', 'pdf:docinfo:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:docinfo:subject': '-  Computing methodologies  ->  Natural language processing.', 'pdf:docinfo:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'pdf:docinfo:trapped': 'False', 'pdf:encrypted': 'false', 'pdf:hasCollection': 'false', 'pdf:hasMarkedContent': 'false', 'pdf:hasXFA': 'false', 'pdf:hasXMP': 'true', 'pdf:num3DAnnotations': '0', 'pdf:overallPercentageUnmappedUnicodeChars': '7.54147840780206E-5', 'pdf:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:totalUnmappedUnicodeChars': '7', 'pdf:unmappedUnicodeCharsPerPage': ['0', '0', '7', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], 'xmp:CreatorTool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'xmpTPg:NPages': '14'}, tag_set=None, time_stamp=None, tracking_id=None, additional_properties={})], score=0.785889744758606, additional_properties={}), ScoreChunkDTO(metadata=[ChunkMetadataWithFileData(content=' In addition to these calls for documentation and technical fixes, Bietti and Vatanparast underscore the need for social and political engagement in shaping a future where data driven systems have minimal negative impact on the environment [16]. While [129] benchmarks the training process in a research set- ting, many LMs are deployed in industrial or other settings where the cost of inference might greatly outweigh that of training in the long run. In this scenario, it may be more appropriate to de- ploy models with lower energy costs during inference even if their training costs are high. In addition to benchmarking tools, works estimating the cost increase associated with the introduction of LMs for particular applications, and how they compare to alternative NLP methods, will be important for understanding the trade-offs. When we perform risk/benefit analyses of language technology, we must keep in mind how the risks and benefits are distributed, because they do not accrue to the same people. On the one hand, it is well documented in the literature on environmental racism that the negative effects of climate change are reaching and impacting the world’s most marginalized communities first [1, 27].6 Is it fair or just to ask, for example, that the residents of the Maldives (likely to be underwater by 2100 [6]) or the 800,000 people in Sudan affected 4https://news.org/sustainabledevelopment/blog/2016/10/report-inequalities- exacerbate-climate-impacts-on-poor/ 612 https://ourworldindata.', created_at=datetime.datetime(2024, 2, 14, 8, 52, 44, 689806), id='34811804-11ba-46ee-b0e3-19912b1092c2', qdrant_point_id='8916f428-a02f-4753-a015-d739b6545eaf', updated_at=datetime.datetime(2024, 2, 14, 8, 52, 44, 689807), weight=1.0, chunk_html='<div><div> In addition to these calls for documentation and technical fixes, Bietti and Vatanparast underscore the need for social and political engagement in shaping a future where data driven systems have minimal negative impact on the environment [16]. While [129] benchmarks the training process in a research set- ting,<b> many LMs are deployed in industrial or other settings where the cost of inference might greatly outweigh that of training in the long run.</b> In this scenario, it may be more appropriate to de- ploy models with lower energy costs during inference even if their training costs are high. In addition to benchmarking tools, works estimating the cost increase associated with the introduction of LMs for particular applications, and how they compare to alternative NLP methods, will be important for understanding the trade-offs. When we perform risk/benefit analyses of language technology,<b> we must keep in mind how the risks and benefits are distributed,</b> because they do not accrue to the same people. On the one hand,<b> it is well documented in the literature on environmental racism that the negative effects of climate change are reaching and impacting the world’s most marginalized communities first [1,</b> 27].6 Is it fair or just to ask, for example, that the residents of the Maldives (likely to be underwater by 2100 [6]) or the 800,000 people in Sudan affected 4https://news.org/sustainabledevelopment/blog/2016/10/report-inequalities- exacerbate-climate-impacts-on-poor/ 612 https://ourworldindata.</div></div>', file_id='05eebb93-b71a-42a6-99c3-ae0c546c4f12', file_name='stochastic_parrots.pdf', link=None, metadata={'Content-Length': '304490', 'Content-Type': 'application/pdf', 'PDFVersion': '1.5', 'PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'X-TIKA:Parsed-By': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'X-TIKA:Parsed-By-Full-Set': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'access_permission:assemble_document': 'true', 'access_permission:can_modify': 'true', 'access_permission:can_print': 'true', 'access_permission:can_print_degraded': 'true', 'access_permission:extract_content': 'true', 'access_permission:extract_for_accessibility': 'true', 'access_permission:fill_in_form': 'true', 'access_permission:modify_annotations': 'true', 'dc:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'dc:format': 'application/pdf; version=1.6', 'dc:language': 'en', 'dc:subject': '-  Computing methodologies  ->  Natural language processing.', 'dc:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'dcterms:created': '2021-02-09T18:04:12Z', 'dcterms:modified': '2024-02-14T08:52:36Z', 'language': 'en', 'pdf:PDFVersion': '1.6', 'pdf:annotationSubtypes': 'Link', 'pdf:annotationTypes': 'null', 'pdf:charsPerPage': ['4262', '5546', '5999', '6061', '5888', '5961', '5806', '6061', '6022', '6021', '8947', '9205', '9288', '7753'], 'pdf:containsDamagedFont': 'false', 'pdf:containsNonEmbeddedFont': 'false', 'pdf:docinfo:created': '2021-02-09T18:04:12Z', 'pdf:docinfo:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'pdf:docinfo:creator_tool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'pdf:docinfo:custom:PDFVersion': '1.5', 'pdf:docinfo:custom:PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'pdf:docinfo:modified': '2024-02-14T08:52:36Z', 'pdf:docinfo:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:docinfo:subject': '-  Computing methodologies  ->  Natural language processing.', 'pdf:docinfo:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'pdf:docinfo:trapped': 'False', 'pdf:encrypted': 'false', 'pdf:hasCollection': 'false', 'pdf:hasMarkedContent': 'false', 'pdf:hasXFA': 'false', 'pdf:hasXMP': 'true', 'pdf:num3DAnnotations': '0', 'pdf:overallPercentageUnmappedUnicodeChars': '7.54147840780206E-5', 'pdf:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:totalUnmappedUnicodeChars': '7', 'pdf:unmappedUnicodeCharsPerPage': ['0', '0', '7', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], 'xmp:CreatorTool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'xmpTPg:NPages': '14'}, tag_set=None, time_stamp=None, tracking_id=None, additional_properties={})], score=0.7666579484939575, additional_properties={}), ScoreChunkDTO(metadata=[ChunkMetadataWithFileData(content=' One way this can be done is by reporting costs and evaluating works based on the amount of resources they con- sume [57]. As we outline in §3, increasing the environmental and financial costs of these models doubly punishes marginalized com- munities that are least likely to benefit from the progress achieved by large LMs and most likely to be harmed by negative environ- mental consequences of its resource consumption. At the scale we are discussing (outlined in §2), the first consideration should be the environmental cost. Just as environmental impact scales with model size, so does the difficulty of understanding what is in the training data. In §4, we discuss how large datasets based on texts from the Internet overrepresent hegemonic viewpoints and encode biases potentially damaging to marginalized populations. In collecting ever larger datasets we risk incurring documentation debt. We recommend mitigating these risks by budgeting for curation and documentation at the start of a project and only creating datasets as large as can be sufficiently documented. As argued by Bender and Koller [14], it is important to under- stand the limitations of LMs and put their success in context. This not only helps reduce hype which can mislead the public and re- searchers themselves regarding the capabilities of these LMs, but might encourage new research directions that do not necessarily depend on having larger LMs. As we discuss in §5, LMs are not performing natural language understanding (NLU), and only have success in tasks that can be approached by manipulating linguis- tic form [14]. Focusing on state-of-the-art results on leaderboards without encouraging deeper understanding of the mechanism by which they are achieved can cause misleading results as shown 610 This work is licensed under a Creative Commons Attribution International 4.', created_at=datetime.datetime(2024, 2, 14, 8, 52, 42, 886996), id='aefc83ac-4ddc-4650-b5aa-50cdf605200e', qdrant_point_id='1095256c-d678-4607-8649-d7623dbb74f5', updated_at=datetime.datetime(2024, 2, 14, 8, 52, 42, 886997), weight=1.0, chunk_html='<b><div><div> One way this can be done is by reporting costs and evaluating works based on the amount of resources they con- sume [57].</b> As we outline in §3,<b> increasing the environmental and financial costs of these models doubly punishes marginalized com- munities that are least likely to benefit from the progress achieved by large LMs and most likely to be harmed by negative environ- mental consequences of its resource consumption.</b> At the scale we are discussing (outlined in §2), the first consideration should be the environmental cost. Just as environmental impact scales with model size, so does the difficulty of understanding what is in the training data. In §4, we discuss how large datasets based on texts from the Internet overrepresent hegemonic viewpoints and encode biases potentially damaging to marginalized populations. In collecting ever larger datasets we risk incurring documentation debt. We recommend mitigating these risks by budgeting for curation and documentation at the start of a project and only creating datasets as large as can be sufficiently documented. As argued by Bender and Koller [14], it is important to under- stand the limitations of LMs and put their success in context.<b> This not only helps reduce hype which can mislead the public and re- searchers themselves regarding the capabilities of these LMs,</b> but might encourage new research directions that do not necessarily depend on having larger LMs. As we discuss in §5, LMs are not performing natural language understanding (NLU), and only have success in tasks that can be approached by manipulating linguis- tic form [14]. Focusing on state-of-the-art results on leaderboards without encouraging deeper understanding of the mechanism by which they are achieved can cause misleading results as shown 610 This work is licensed under a Creative Commons Attribution International 4.</div></div>', file_id='05eebb93-b71a-42a6-99c3-ae0c546c4f12', file_name='stochastic_parrots.pdf', link=None, metadata={'Content-Length': '304490', 'Content-Type': 'application/pdf', 'PDFVersion': '1.5', 'PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'X-TIKA:Parsed-By': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'X-TIKA:Parsed-By-Full-Set': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'access_permission:assemble_document': 'true', 'access_permission:can_modify': 'true', 'access_permission:can_print': 'true', 'access_permission:can_print_degraded': 'true', 'access_permission:extract_content': 'true', 'access_permission:extract_for_accessibility': 'true', 'access_permission:fill_in_form': 'true', 'access_permission:modify_annotations': 'true', 'dc:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'dc:format': 'application/pdf; version=1.6', 'dc:language': 'en', 'dc:subject': '-  Computing methodologies  ->  Natural language processing.', 'dc:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'dcterms:created': '2021-02-09T18:04:12Z', 'dcterms:modified': '2024-02-14T08:52:36Z', 'language': 'en', 'pdf:PDFVersion': '1.6', 'pdf:annotationSubtypes': 'Link', 'pdf:annotationTypes': 'null', 'pdf:charsPerPage': ['4262', '5546', '5999', '6061', '5888', '5961', '5806', '6061', '6022', '6021', '8947', '9205', '9288', '7753'], 'pdf:containsDamagedFont': 'false', 'pdf:containsNonEmbeddedFont': 'false', 'pdf:docinfo:created': '2021-02-09T18:04:12Z', 'pdf:docinfo:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'pdf:docinfo:creator_tool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'pdf:docinfo:custom:PDFVersion': '1.5', 'pdf:docinfo:custom:PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'pdf:docinfo:modified': '2024-02-14T08:52:36Z', 'pdf:docinfo:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:docinfo:subject': '-  Computing methodologies  ->  Natural language processing.', 'pdf:docinfo:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'pdf:docinfo:trapped': 'False', 'pdf:encrypted': 'false', 'pdf:hasCollection': 'false', 'pdf:hasMarkedContent': 'false', 'pdf:hasXFA': 'false', 'pdf:hasXMP': 'true', 'pdf:num3DAnnotations': '0', 'pdf:overallPercentageUnmappedUnicodeChars': '7.54147840780206E-5', 'pdf:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:totalUnmappedUnicodeChars': '7', 'pdf:unmappedUnicodeCharsPerPage': ['0', '0', '7', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], 'xmp:CreatorTool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'xmpTPg:NPages': '14'}, tag_set=None, time_stamp=None, tracking_id=None, additional_properties={})], score=0.7633704543113708, additional_properties={}), ScoreChunkDTO(metadata=[ChunkMetadataWithFileData(content=' ACM Reference Format: Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmar- garet Shmitchell. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Conference on Fairness, Accountability, and Trans- parency (FAccT ’21), March 3–10, 2021, Virtual Event, Canada. ACM, New York, NY, USA, 14 pages.3445922 1 INTRODUCTION One of the biggest trends in natural language processing (NLP) has been the increasing size of language models (LMs) as measured by the number of parameters and size of training data. Since 2018 ∗Joint first authors FAccT ’21, March 3–10, 2021, Virtual Event, Canada ACM ISBN 978-1-4503-8309-7/21/03.3445922 alone, we have seen the emergence of BERT and its variants [39, 70, 74, 113, 146], GPT-2 [106], T-NLG [112], GPT-3 [25], and most recently Switch-C [43], with institutions seemingly competing to produce ever larger LMs. While investigating properties of LMs and how they change with size holds scientific interest, and large LMs have shown improvements on various tasks (§2), we ask whether enough thought has been put into the potential risks associated with developing them and strategies to mitigate these risks. We first consider environmental risks. Echoing a line of recent work outlining the environmental and financial costs of deep learn- ing systems [129], we encourage the research community to priori- tize these impacts.', created_at=datetime.datetime(2024, 2, 14, 8, 52, 42, 630919), id='57fd9409-b273-43df-aa27-fafd1b940ac4', qdrant_point_id='387083ee-57fd-4ffa-921b-ccee6d67dc2c', updated_at=datetime.datetime(2024, 2, 14, 8, 52, 42, 630921), weight=1.0, chunk_html='<div><div> ACM Reference Format: Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmar- garet Shmitchell. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Conference on Fairness, Accountability, and Trans- parency (FAccT ’21), March 3–10, 2021, Virtual Event, Canada. ACM, New York, NY, USA, 14 pages.<b>3445922 1 INTRODUCTION One of the biggest trends in natural language processing (NLP) has been the increasing size of language models (LMs) as measured by the number of parameters and size of training data.</b> Since 2018 ∗Joint first authors FAccT ’21, March 3–10, 2021, Virtual Event, Canada ACM ISBN 978-1-4503-8309-7/21/03.3445922 alone, we have seen the emergence of BERT and its variants [39, 70, 74, 113, 146], GPT-2 [106], T-NLG [112], GPT-3 [25], and most recently Switch-C [43], with institutions seemingly competing to produce ever larger LMs. While investigating properties of LMs and how they change with size holds scientific interest, and large LMs have shown improvements on various tasks (§2),<b> we ask whether enough thought has been put into the potential risks associated with developing them and strategies to mitigate these risks.</b> We first consider environmental risks.<b> Echoing a line of recent work outlining the environmental and financial costs of deep learn- ing systems [129],</b> we encourage the research community to priori- tize these impacts.</div></div>', file_id='05eebb93-b71a-42a6-99c3-ae0c546c4f12', file_name='stochastic_parrots.pdf', link=None, metadata={'Content-Length': '304490', 'Content-Type': 'application/pdf', 'PDFVersion': '1.5', 'PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'X-TIKA:Parsed-By': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'X-TIKA:Parsed-By-Full-Set': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'access_permission:assemble_document': 'true', 'access_permission:can_modify': 'true', 'access_permission:can_print': 'true', 'access_permission:can_print_degraded': 'true', 'access_permission:extract_content': 'true', 'access_permission:extract_for_accessibility': 'true', 'access_permission:fill_in_form': 'true', 'access_permission:modify_annotations': 'true', 'dc:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'dc:format': 'application/pdf; version=1.6', 'dc:language': 'en', 'dc:subject': '-  Computing methodologies  ->  Natural language processing.', 'dc:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'dcterms:created': '2021-02-09T18:04:12Z', 'dcterms:modified': '2024-02-14T08:52:36Z', 'language': 'en', 'pdf:PDFVersion': '1.6', 'pdf:annotationSubtypes': 'Link', 'pdf:annotationTypes': 'null', 'pdf:charsPerPage': ['4262', '5546', '5999', '6061', '5888', '5961', '5806', '6061', '6022', '6021', '8947', '9205', '9288', '7753'], 'pdf:containsDamagedFont': 'false', 'pdf:containsNonEmbeddedFont': 'false', 'pdf:docinfo:created': '2021-02-09T18:04:12Z', 'pdf:docinfo:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'pdf:docinfo:creator_tool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'pdf:docinfo:custom:PDFVersion': '1.5', 'pdf:docinfo:custom:PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'pdf:docinfo:modified': '2024-02-14T08:52:36Z', 'pdf:docinfo:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:docinfo:subject': '-  Computing methodologies  ->  Natural language processing.', 'pdf:docinfo:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'pdf:docinfo:trapped': 'False', 'pdf:encrypted': 'false', 'pdf:hasCollection': 'false', 'pdf:hasMarkedContent': 'false', 'pdf:hasXFA': 'false', 'pdf:hasXMP': 'true', 'pdf:num3DAnnotations': '0', 'pdf:overallPercentageUnmappedUnicodeChars': '7.54147840780206E-5', 'pdf:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:totalUnmappedUnicodeChars': '7', 'pdf:unmappedUnicodeCharsPerPage': ['0', '0', '7', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], 'xmp:CreatorTool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'xmpTPg:NPages': '14'}, tag_set=None, time_stamp=None, tracking_id=None, additional_properties={})], score=0.7628223896026611, additional_properties={}), ScoreChunkDTO(metadata=[ChunkMetadataWithFileData(content=' On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?edu University of Washington Seattle, WA, USA Timnit Gebru∗ timnit@blackinai.org Black in AI Palo Alto, CA, USA Angelina McMillan-Major aymm@uw.edu University of Washington Seattle, WA, USA Shmargaret Shmitchell shmargaret.com The Aether ABSTRACT The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, es- pecially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmen- tal and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models. CCS CONCEPTS •Computingmethodologies→Natural language processing.', created_at=datetime.datetime(2024, 2, 14, 8, 52, 42, 497562), id='6959558b-00c7-45cb-8a5a-95c2a41dd8a1', qdrant_point_id='1932708e-5807-458a-9d50-87a04fefb855', updated_at=datetime.datetime(2024, 2, 14, 8, 52, 42, 497565), weight=1.0, chunk_html='<div><div> On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?edu University of Washington Seattle, WA, USA Timnit Gebru∗ timnit@blackinai.org Black in AI Palo Alto, CA, USA Angelina McMillan-Major aymm@uw.edu University of Washington Seattle, WA, USA Shmargaret Shmitchell shmargaret.<b>com The Aether ABSTRACT The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models,</b> es- pecially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size.<b> Using these pretrained models and the methodology of fine-tuning them for specific tasks,</b> researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big?<b> What are the possible risks associated with this technology and what paths are available for mitigating those risks?</b> We provide recommendations including weighing the environmen- tal and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models. CCS CONCEPTS •Computingmethodologies→Natural language processing.</div></div>', file_id='05eebb93-b71a-42a6-99c3-ae0c546c4f12', file_name='stochastic_parrots.pdf', link=None, metadata={'Content-Length': '304490', 'Content-Type': 'application/pdf', 'PDFVersion': '1.5', 'PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'X-TIKA:Parsed-By': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'X-TIKA:Parsed-By-Full-Set': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'access_permission:assemble_document': 'true', 'access_permission:can_modify': 'true', 'access_permission:can_print': 'true', 'access_permission:can_print_degraded': 'true', 'access_permission:extract_content': 'true', 'access_permission:extract_for_accessibility': 'true', 'access_permission:fill_in_form': 'true', 'access_permission:modify_annotations': 'true', 'dc:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'dc:format': 'application/pdf; version=1.6', 'dc:language': 'en', 'dc:subject': '-  Computing methodologies  ->  Natural language processing.', 'dc:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'dcterms:created': '2021-02-09T18:04:12Z', 'dcterms:modified': '2024-02-14T08:52:36Z', 'language': 'en', 'pdf:PDFVersion': '1.6', 'pdf:annotationSubtypes': 'Link', 'pdf:annotationTypes': 'null', 'pdf:charsPerPage': ['4262', '5546', '5999', '6061', '5888', '5961', '5806', '6061', '6022', '6021', '8947', '9205', '9288', '7753'], 'pdf:containsDamagedFont': 'false', 'pdf:containsNonEmbeddedFont': 'false', 'pdf:docinfo:created': '2021-02-09T18:04:12Z', 'pdf:docinfo:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'pdf:docinfo:creator_tool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'pdf:docinfo:custom:PDFVersion': '1.5', 'pdf:docinfo:custom:PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'pdf:docinfo:modified': '2024-02-14T08:52:36Z', 'pdf:docinfo:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:docinfo:subject': '-  Computing methodologies  ->  Natural language processing.', 'pdf:docinfo:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'pdf:docinfo:trapped': 'False', 'pdf:encrypted': 'false', 'pdf:hasCollection': 'false', 'pdf:hasMarkedContent': 'false', 'pdf:hasXFA': 'false', 'pdf:hasXMP': 'true', 'pdf:num3DAnnotations': '0', 'pdf:overallPercentageUnmappedUnicodeChars': '7.54147840780206E-5', 'pdf:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:totalUnmappedUnicodeChars': '7', 'pdf:unmappedUnicodeCharsPerPage': ['0', '0', '7', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], 'xmp:CreatorTool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'xmpTPg:NPages': '14'}, tag_set=None, time_stamp=None, tracking_id=None, additional_properties={})], score=0.7614707946777344, additional_properties={}), ScoreChunkDTO(metadata=[ChunkMetadataWithFileData(content=' In addition to ASR, these large n-gram models of English were developed in the context of machine translation from another source language with far fewer direct translation examples. For example, [20] developed an n-gram model for English with a total of 1.8T n-grams and noted steady improvements in BLEU score on the test set of 1797 Arabic translations as the training data was increased from 13M tokens. The next big step was the move towards using pretrained rep- resentations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entail- ment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in Eng- lish and later for other languages as well. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached Year Model # of Parameters Dataset Size 2019 BERT [39] 3.4E+08 16GB 2019 DistilBERT [113] 6.60E+07 16GB 2019 ALBERT [70] 2.23E+08 16GB 2019 XLNet (Large) [150] 3.40E+08 126GB 2020 ERNIE-Gen (Large) [145] 3.40E+08 16GB 2019 RoBERTa (Large) [74] 3.', created_at=datetime.datetime(2024, 2, 14, 8, 52, 43, 245836), id='1fb9e6f5-33e1-47f7-9ad1-b179c50e6872', qdrant_point_id='c2770845-5c15-4cb9-8fcc-a16d8e1729f3', updated_at=datetime.datetime(2024, 2, 14, 8, 52, 43, 245837), weight=1.0, chunk_html='<div><div> In addition to ASR,<b> these large n-gram models of English were developed in the context of machine translation from another source language with far fewer direct translation examples.</b> For example, [20] developed an n-gram model for English with a total of 1.<b>8T n-grams and noted steady improvements in BLEU score on the test set of 1797 Arabic translations as the training data was increased from 13M tokens.</b> The next big step was the move towards using pretrained rep- resentations of the distribution of words (called word embeddings) in other (supervised) NLP tasks.<b> These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering,</b> textual entail- ment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in Eng- lish and later for other languages as well. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached Year Model # of Parameters Dataset Size 2019 BERT [39] 3.4E+08 16GB 2019 DistilBERT [113] 6.60E+07 16GB 2019 ALBERT [70] 2.23E+08 16GB 2019 XLNet (Large) [150] 3.40E+08 126GB 2020 ERNIE-Gen (Large) [145] 3.40E+08 16GB 2019 RoBERTa (Large) [74] 3.</div></div>', file_id='05eebb93-b71a-42a6-99c3-ae0c546c4f12', file_name='stochastic_parrots.pdf', link=None, metadata={'Content-Length': '304490', 'Content-Type': 'application/pdf', 'PDFVersion': '1.5', 'PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'X-TIKA:Parsed-By': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'X-TIKA:Parsed-By-Full-Set': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'access_permission:assemble_document': 'true', 'access_permission:can_modify': 'true', 'access_permission:can_print': 'true', 'access_permission:can_print_degraded': 'true', 'access_permission:extract_content': 'true', 'access_permission:extract_for_accessibility': 'true', 'access_permission:fill_in_form': 'true', 'access_permission:modify_annotations': 'true', 'dc:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'dc:format': 'application/pdf; version=1.6', 'dc:language': 'en', 'dc:subject': '-  Computing methodologies  ->  Natural language processing.', 'dc:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'dcterms:created': '2021-02-09T18:04:12Z', 'dcterms:modified': '2024-02-14T08:52:36Z', 'language': 'en', 'pdf:PDFVersion': '1.6', 'pdf:annotationSubtypes': 'Link', 'pdf:annotationTypes': 'null', 'pdf:charsPerPage': ['4262', '5546', '5999', '6061', '5888', '5961', '5806', '6061', '6022', '6021', '8947', '9205', '9288', '7753'], 'pdf:containsDamagedFont': 'false', 'pdf:containsNonEmbeddedFont': 'false', 'pdf:docinfo:created': '2021-02-09T18:04:12Z', 'pdf:docinfo:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'pdf:docinfo:creator_tool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'pdf:docinfo:custom:PDFVersion': '1.5', 'pdf:docinfo:custom:PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'pdf:docinfo:modified': '2024-02-14T08:52:36Z', 'pdf:docinfo:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:docinfo:subject': '-  Computing methodologies  ->  Natural language processing.', 'pdf:docinfo:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'pdf:docinfo:trapped': 'False', 'pdf:encrypted': 'false', 'pdf:hasCollection': 'false', 'pdf:hasMarkedContent': 'false', 'pdf:hasXFA': 'false', 'pdf:hasXMP': 'true', 'pdf:num3DAnnotations': '0', 'pdf:overallPercentageUnmappedUnicodeChars': '7.54147840780206E-5', 'pdf:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:totalUnmappedUnicodeChars': '7', 'pdf:unmappedUnicodeCharsPerPage': ['0', '0', '7', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], 'xmp:CreatorTool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'xmpTPg:NPages': '14'}, tag_set=None, time_stamp=None, tracking_id=None, additional_properties={})], score=0.7607405781745911, additional_properties={}), ScoreChunkDTO(metadata=[ChunkMetadataWithFileData(content='55E+08 161GB 2019 MegatronLM [122] 8.30E+09 174GB 2020 T5-11B [107] 1.10E+10 745GB 2020 T-NLG [112] 1.70E+10 174GB 2020 GPT-3 [25] 1.75E+11 570GB 2020 GShard [73] 6.00E+11 – 2021 Switch-C [43] 1.57E+12 745GB Table 1: Overview of recent large language models the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e. Transformer models, on the other hand, have been able to con- tinuously benefit from larger architectures and larger quantities of data. Devlin et al. [39] in particular noted that training on a large dataset and fine-tuning for specific tasks leads to strictly increasing results on the GLUE tasks [138] for English as the hyperparameters of the model were increased.', created_at=datetime.datetime(2024, 2, 14, 8, 52, 43, 424189), id='5990be40-9e29-4c0b-8032-d0762c5d3ec8', qdrant_point_id='1bfe61a6-e30a-48cf-ac16-14ff279c37d9', updated_at=datetime.datetime(2024, 2, 14, 8, 52, 43, 424191), weight=1.0, chunk_html='<div><div>55E+08 161GB 2019 MegatronLM [122] 8.30E+09 174GB 2020 T5-11B [107] 1.10E+10 745GB 2020 T-NLG [112] 1.70E+10 174GB 2020 GPT-3 [25] 1.75E+11 570GB 2020 GShard [73] 6.00E+11 – 2021 Switch-C [43] 1.<b>57E+12 745GB Table 1: Overview of recent large language models the maximum development F1 score in 10 epochs as opposed to 486 without ELMo.</b><b> This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data.</b> Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e. Transformer models, on the other hand, have been able to con- tinuously benefit from larger architectures and larger quantities of data. Devlin et al.<b> [39] in particular noted that training on a large dataset and fine-tuning for specific tasks leads to strictly increasing results on the GLUE tasks [138] for English as the hyperparameters of the model were increased.</b></div></div>', file_id='05eebb93-b71a-42a6-99c3-ae0c546c4f12', file_name='stochastic_parrots.pdf', link=None, metadata={'Content-Length': '304490', 'Content-Type': 'application/pdf', 'PDFVersion': '1.5', 'PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'X-TIKA:Parsed-By': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'X-TIKA:Parsed-By-Full-Set': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'access_permission:assemble_document': 'true', 'access_permission:can_modify': 'true', 'access_permission:can_print': 'true', 'access_permission:can_print_degraded': 'true', 'access_permission:extract_content': 'true', 'access_permission:extract_for_accessibility': 'true', 'access_permission:fill_in_form': 'true', 'access_permission:modify_annotations': 'true', 'dc:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'dc:format': 'application/pdf; version=1.6', 'dc:language': 'en', 'dc:subject': '-  Computing methodologies  ->  Natural language processing.', 'dc:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'dcterms:created': '2021-02-09T18:04:12Z', 'dcterms:modified': '2024-02-14T08:52:36Z', 'language': 'en', 'pdf:PDFVersion': '1.6', 'pdf:annotationSubtypes': 'Link', 'pdf:annotationTypes': 'null', 'pdf:charsPerPage': ['4262', '5546', '5999', '6061', '5888', '5961', '5806', '6061', '6022', '6021', '8947', '9205', '9288', '7753'], 'pdf:containsDamagedFont': 'false', 'pdf:containsNonEmbeddedFont': 'false', 'pdf:docinfo:created': '2021-02-09T18:04:12Z', 'pdf:docinfo:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'pdf:docinfo:creator_tool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'pdf:docinfo:custom:PDFVersion': '1.5', 'pdf:docinfo:custom:PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'pdf:docinfo:modified': '2024-02-14T08:52:36Z', 'pdf:docinfo:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:docinfo:subject': '-  Computing methodologies  ->  Natural language processing.', 'pdf:docinfo:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'pdf:docinfo:trapped': 'False', 'pdf:encrypted': 'false', 'pdf:hasCollection': 'false', 'pdf:hasMarkedContent': 'false', 'pdf:hasXFA': 'false', 'pdf:hasXMP': 'true', 'pdf:num3DAnnotations': '0', 'pdf:overallPercentageUnmappedUnicodeChars': '7.54147840780206E-5', 'pdf:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:totalUnmappedUnicodeChars': '7', 'pdf:unmappedUnicodeCharsPerPage': ['0', '0', '7', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], 'xmp:CreatorTool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'xmpTPg:NPages': '14'}, tag_set=None, time_stamp=None, tracking_id=None, additional_properties={})], score=0.7282400131225586, additional_properties={}), ScoreChunkDTO(metadata=[ChunkMetadataWithFileData(content=' Initially developed as Chinese LMs, the ERNIE family [130, 131, 145] produced ERNIE-Gen, which was also trained on the original (English) BERT dataset, joining the ranks of very large LMs. NVIDIA released the MegatronLM which has 8.3B parameters and was trained on 174GB of text from the English Wikipedia, OpenWebText, RealNews and CC-Stories datasets [122]. Trained on the same dataset, Microsoft released T-NLG,1 an LM with 17B parameters. OpenAI’s GPT-3 [25] and Google’s GShard [73] and Switch-C [43] have increased the definition of large LM by orders of magnitude in terms of parameters at 175B, 600B, and 1. Table 1 summarizes a selection of these LMs in terms of training data size and parameters. As increasingly large amounts of text are collected from the web in datasets such as the Colossal Clean Crawled Corpus [107] and the Pile [51], this trend of increasingly large LMs can be expected to continue as long as they correlate with an increase in performance. A number of these models also have multilingual variants such as mBERT [39] and mT5 [148] or are trained with some amount of multilingual data such as GPT-3 where 7% of the training data was not in English [25]. The performance of these multilingual mod- els across languages is an active area of research. Wu and Drezde [144] found that while mBERT does not perform equally well across all 104 languages in its training data, it performed better at NER, POS tagging, and dependency parsing than monolingual models trained with comparable amounts of data for four low-resource languages. Conversely, [95] surveyed monolingual BERT models developed with more specific architecture considerations or addi- tional monolingual data and found that they generally outperform 1https://www.com/en-us/research/blog/turing-nlg-a-17-billion-parameter- language-model-by-microsoft/ 611', created_at=datetime.datetime(2024, 2, 14, 8, 52, 43, 660871), id='d37c0129-b9b7-4efb-95e7-3516af7eb447', qdrant_point_id='be222afb-ffbe-4468-9b5d-52232e65ae73', updated_at=datetime.datetime(2024, 2, 14, 8, 52, 43, 660872), weight=1.0, chunk_html='<div><div> Initially developed as Chinese LMs, the ERNIE family [130, 131, 145] produced ERNIE-Gen, which was also trained on the original (English) BERT dataset, joining the ranks of very large LMs. NVIDIA released the MegatronLM which has 8.3B parameters and was trained on 174GB of text from the English Wikipedia, OpenWebText, RealNews and CC-Stories datasets [122]. Trained on the same dataset, Microsoft released T-NLG,1 an LM with 17B parameters. OpenAI’s GPT-3 [25] and Google’s GShard [73] and Switch-C [43] have increased the definition of large LM by orders of magnitude in terms of parameters at 175B, 600B, and 1. Table 1 summarizes a selection of these LMs in terms of training data size and parameters.<b> As increasingly large amounts of text are collected from the web in datasets such as the Colossal Clean Crawled Corpus [107] and the Pile [51],</b><b> this trend of increasingly large LMs can be expected to continue as long as they correlate with an increase in performance.</b><b> A number of these models also have multilingual variants such as mBERT [39] and mT5 [148] or are trained with some amount of multilingual data such as GPT-3 where 7% of the training data was not in English [25].</b> The performance of these multilingual mod- els across languages is an active area of research. Wu and Drezde [144] found that while mBERT does not perform equally well across all 104 languages in its training data, it performed better at NER, POS tagging, and dependency parsing than monolingual models trained with comparable amounts of data for four low-resource languages. Conversely, [95] surveyed monolingual BERT models developed with more specific architecture considerations or addi- tional monolingual data and found that they generally outperform 1https://www.com/en-us/research/blog/turing-nlg-a-17-billion-parameter- language-model-by-microsoft/ 611 </div></div>', file_id='05eebb93-b71a-42a6-99c3-ae0c546c4f12', file_name='stochastic_parrots.pdf', link=None, metadata={'Content-Length': '304490', 'Content-Type': 'application/pdf', 'PDFVersion': '1.5', 'PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'X-TIKA:Parsed-By': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'X-TIKA:Parsed-By-Full-Set': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'access_permission:assemble_document': 'true', 'access_permission:can_modify': 'true', 'access_permission:can_print': 'true', 'access_permission:can_print_degraded': 'true', 'access_permission:extract_content': 'true', 'access_permission:extract_for_accessibility': 'true', 'access_permission:fill_in_form': 'true', 'access_permission:modify_annotations': 'true', 'dc:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'dc:format': 'application/pdf; version=1.6', 'dc:language': 'en', 'dc:subject': '-  Computing methodologies  ->  Natural language processing.', 'dc:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'dcterms:created': '2021-02-09T18:04:12Z', 'dcterms:modified': '2024-02-14T08:52:36Z', 'language': 'en', 'pdf:PDFVersion': '1.6', 'pdf:annotationSubtypes': 'Link', 'pdf:annotationTypes': 'null', 'pdf:charsPerPage': ['4262', '5546', '5999', '6061', '5888', '5961', '5806', '6061', '6022', '6021', '8947', '9205', '9288', '7753'], 'pdf:containsDamagedFont': 'false', 'pdf:containsNonEmbeddedFont': 'false', 'pdf:docinfo:created': '2021-02-09T18:04:12Z', 'pdf:docinfo:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'pdf:docinfo:creator_tool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'pdf:docinfo:custom:PDFVersion': '1.5', 'pdf:docinfo:custom:PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'pdf:docinfo:modified': '2024-02-14T08:52:36Z', 'pdf:docinfo:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:docinfo:subject': '-  Computing methodologies  ->  Natural language processing.', 'pdf:docinfo:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'pdf:docinfo:trapped': 'False', 'pdf:encrypted': 'false', 'pdf:hasCollection': 'false', 'pdf:hasMarkedContent': 'false', 'pdf:hasXFA': 'false', 'pdf:hasXMP': 'true', 'pdf:num3DAnnotations': '0', 'pdf:overallPercentageUnmappedUnicodeChars': '7.54147840780206E-5', 'pdf:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:totalUnmappedUnicodeChars': '7', 'pdf:unmappedUnicodeCharsPerPage': ['0', '0', '7', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], 'xmp:CreatorTool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'xmpTPg:NPages': '14'}, tag_set=None, time_stamp=None, tracking_id=None, additional_properties={})], score=0.7193779945373535, additional_properties={}), ScoreChunkDTO(metadata=[ChunkMetadataWithFileData(content='14m-trees-cut-scotland-make-way- wind-farms/ green energy,4 underscoring the need for energy efficient model architectures and training paradigms. Strubell et al. also examine the cost of these models vs. their accuracy gains. For the task of machine translation where large LMs have resulted in performance gains, they estimate that an increase in 0.1 BLEU score using neural architecture search for English to German translation results in an increase of $150,000 compute cost in addition to the carbon emissions. To encourage more equitable access to NLP research and reduce carbon footprint, the authors give recommendations to report training time and sensitivity to hyperparameters when the released model is meant to be re-trained for downstream use.They also urge governments to invest in compute clouds to provide equitable access to researchers.', created_at=datetime.datetime(2024, 2, 14, 8, 52, 44, 356915), id='02230eb6-11d2-4c92-aea3-7059d2f6173a', qdrant_point_id='79cca69c-0a6c-4d03-abcc-be67b589a4c3', updated_at=datetime.datetime(2024, 2, 14, 8, 52, 44, 356917), weight=1.0, chunk_html='<div><div>14m-trees-cut-scotland-make-way- wind-farms/ green energy,4 underscoring the need for energy efficient model architectures and training paradigms. Strubell et al.<b> also examine the cost of these models vs.</b> their accuracy gains.<b> For the task of machine translation where large LMs have resulted in performance gains,</b> they estimate that an increase in 0.<b>1 BLEU score using neural architecture search for English to German translation results in an increase of $150,</b>000 compute cost in addition to the carbon emissions. To encourage more equitable access to NLP research and reduce carbon footprint, the authors give recommendations to report training time and sensitivity to hyperparameters when the released model is meant to be re-trained for downstream use.They also urge governments to invest in compute clouds to provide equitable access to researchers.</div></div>', file_id='05eebb93-b71a-42a6-99c3-ae0c546c4f12', file_name='stochastic_parrots.pdf', link=None, metadata={'Content-Length': '304490', 'Content-Type': 'application/pdf', 'PDFVersion': '1.5', 'PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'X-TIKA:Parsed-By': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'X-TIKA:Parsed-By-Full-Set': ['org.apache.tika.parser.DefaultParser', 'org.apache.tika.parser.pdf.PDFParser'], 'access_permission:assemble_document': 'true', 'access_permission:can_modify': 'true', 'access_permission:can_print': 'true', 'access_permission:can_print_degraded': 'true', 'access_permission:extract_content': 'true', 'access_permission:extract_for_accessibility': 'true', 'access_permission:fill_in_form': 'true', 'access_permission:modify_annotations': 'true', 'dc:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'dc:format': 'application/pdf; version=1.6', 'dc:language': 'en', 'dc:subject': '-  Computing methodologies  ->  Natural language processing.', 'dc:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'dcterms:created': '2021-02-09T18:04:12Z', 'dcterms:modified': '2024-02-14T08:52:36Z', 'language': 'en', 'pdf:PDFVersion': '1.6', 'pdf:annotationSubtypes': 'Link', 'pdf:annotationTypes': 'null', 'pdf:charsPerPage': ['4262', '5546', '5999', '6061', '5888', '5961', '5806', '6061', '6022', '6021', '8947', '9205', '9288', '7753'], 'pdf:containsDamagedFont': 'false', 'pdf:containsNonEmbeddedFont': 'false', 'pdf:docinfo:created': '2021-02-09T18:04:12Z', 'pdf:docinfo:creator': 'Emily M. Bender; Timnit Gebru; Angelina McMillan-Major; Shmargaret Shmitchell', 'pdf:docinfo:creator_tool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'pdf:docinfo:custom:PDFVersion': '1.5', 'pdf:docinfo:custom:PTEX.FullBanner': 'This is LuaHBTeX, Version 1.12.0 (TeX Live 2020)', 'pdf:docinfo:modified': '2024-02-14T08:52:36Z', 'pdf:docinfo:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:docinfo:subject': '-  Computing methodologies  ->  Natural language processing.', 'pdf:docinfo:title': 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \"1F99C', 'pdf:docinfo:trapped': 'False', 'pdf:encrypted': 'false', 'pdf:hasCollection': 'false', 'pdf:hasMarkedContent': 'false', 'pdf:hasXFA': 'false', 'pdf:hasXMP': 'true', 'pdf:num3DAnnotations': '0', 'pdf:overallPercentageUnmappedUnicodeChars': '7.54147840780206E-5', 'pdf:producer': 'LuaHBTeX, Version 1.12.0 (TeX Live 2020); modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)', 'pdf:totalUnmappedUnicodeChars': '7', 'pdf:unmappedUnicodeCharsPerPage': ['0', '0', '7', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], 'xmp:CreatorTool': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'xmpTPg:NPages': '14'}, tag_set=None, time_stamp=None, tracking_id=None, additional_properties={})], score=0.7121846079826355, additional_properties={})]\n"
     ]
    }
   ],
   "source": [
    "from trieve_client.api.chunk_group import search_groups\n",
    "from trieve_client.models.search_groups_data import SearchGroupsData\n",
    "from trieve_client.models.search_groups_result import SearchGroupsResult\n",
    "\n",
    "search_groups_response = search_groups.sync(\n",
    "    client=client,\n",
    "    tr_dataset=trieve_dataset_id,\n",
    "    body=SearchGroupsData(\n",
    "        group_id=file_group_id,\n",
    "        search_type=\"semantic\",\n",
    "        query=\"the language models in this scenario are incapable of encapsulating any form of reasoning\"\n",
    "    )\n",
    ")\n",
    "\n",
    "if type(search_groups_response) == SearchGroupsResult:\n",
    "  print(search_groups_response.bookmarks)\n",
    "else:\n",
    "  print(f\"Failed to search groups: {search_groups_response.message}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
